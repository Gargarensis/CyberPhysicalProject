{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished feature extraction from  1521  files\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def extract_features(file_name):\n",
    "\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast')\n",
    "        audio = librosa.resample(audio, sample_rate, 16000)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=128)\n",
    "        mfccsscaled = np.mean(mfccs.T,axis=0)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file)\n",
    "        return None\n",
    "\n",
    "    return mfccsscaled\n",
    "\n",
    "\n",
    "# Load various imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "# Set the path to the full UrbanSound dataset\n",
    "fulldatasetpath = './audio/'\n",
    "\n",
    "# metadata = pd.read_csv(fulldatasetpath + '../metadata/UrbanSound8K.csv')\n",
    "\n",
    "features = []\n",
    "\n",
    "# Iterate through each sound file and extract the features\n",
    "# for index, row in metadata.iterrows():\n",
    "#\n",
    "#     file_name = os.path.join(os.path.abspath(fulldatasetpath),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "#\n",
    "#     class_label = row[\"class_name\"]\n",
    "#     data = extract_features(file_name)\n",
    "#\n",
    "#     features.append([data, class_label])\n",
    "\n",
    "for filename in os.listdir('../footstepss'):\n",
    "    if not filename.endswith('.wav'):\n",
    "        continue;\n",
    "    class_label = \"footstepss\"\n",
    "    data = extract_features('../footstepss/' + filename)\n",
    "    features.append([data, class_label])\n",
    "\n",
    "for filename in os.listdir('../selected'):\n",
    "    if not filename.endswith('.wav'):\n",
    "        continue;\n",
    "    class_label = \"selected\"\n",
    "    data = extract_features('../selected/' + filename)\n",
    "    features.append([data, class_label])\n",
    "\n",
    "\n",
    "# Convert into a Panda dataframe\n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "\n",
    "print('Finished feature extraction from ', len(featuresdf), ' files')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = np.array(featuresdf.feature.tolist())\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y))\n",
    "\n",
    "# split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.1, random_state = 420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1368, 128)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "\n",
    "num_rows = 8\n",
    "num_columns = 16\n",
    "num_channels = 1\n",
    "\n",
    "x_train2 = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "x_test2 = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "filter_size = 2\n",
    "\n",
    "# Construct model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=1, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=1))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=1, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=1))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=1, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=1))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=1, activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=1))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(num_labels, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 8, 16, 16)         32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 8, 16, 16)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 8, 16, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 8, 16, 32)         544       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 8, 16, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 8, 16, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 8, 16, 64)         2112      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 8, 16, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 8, 16, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 8, 16, 128)        8320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 8, 16, 128)        0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 8, 16, 128)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_3 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 11,266\n",
      "Trainable params: 11,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "153/1 [==============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 2ms/sample - loss: 0.8171 - accuracy: 0.4641\n",
      "Pre-training accuracy: 46.4052%\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test2, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1368 samples, validate on 153 samples\n",
      "Epoch 1/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.7515 - accuracy: 0.5109\n",
      "Epoch 00001: val_loss improved from inf to 0.71433, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 1ms/sample - loss: 0.7493 - accuracy: 0.5154 - val_loss: 0.7143 - val_accuracy: 0.5359\n",
      "Epoch 2/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.7002 - accuracy: 0.5422\n",
      "Epoch 00002: val_loss improved from 0.71433 to 0.69051, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 621us/sample - loss: 0.6989 - accuracy: 0.5439 - val_loss: 0.6905 - val_accuracy: 0.4575\n",
      "Epoch 3/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.6992 - accuracy: 0.4867\n",
      "Epoch 00003: val_loss improved from 0.69051 to 0.68791, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 622us/sample - loss: 0.6992 - accuracy: 0.4861 - val_loss: 0.6879 - val_accuracy: 0.5359\n",
      "Epoch 4/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.6877 - accuracy: 0.5625\n",
      "Epoch 00004: val_loss improved from 0.68791 to 0.68784, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 645us/sample - loss: 0.6878 - accuracy: 0.5629 - val_loss: 0.6878 - val_accuracy: 0.5359\n",
      "Epoch 5/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.6819 - accuracy: 0.5734\n",
      "Epoch 00005: val_loss improved from 0.68784 to 0.68204, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 711us/sample - loss: 0.6833 - accuracy: 0.5673 - val_loss: 0.6820 - val_accuracy: 0.5425\n",
      "Epoch 6/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.6815 - accuracy: 0.5555\n",
      "Epoch 00006: val_loss improved from 0.68204 to 0.67664, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 671us/sample - loss: 0.6798 - accuracy: 0.5592 - val_loss: 0.6766 - val_accuracy: 0.5882\n",
      "Epoch 7/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.6758 - accuracy: 0.5703\n",
      "Epoch 00007: val_loss improved from 0.67664 to 0.67389, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 658us/sample - loss: 0.6774 - accuracy: 0.5658 - val_loss: 0.6739 - val_accuracy: 0.5425\n",
      "Epoch 8/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.6698 - accuracy: 0.5953\n",
      "Epoch 00008: val_loss improved from 0.67389 to 0.66409, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 685us/sample - loss: 0.6675 - accuracy: 0.6023 - val_loss: 0.6641 - val_accuracy: 0.6013\n",
      "Epoch 9/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.6643 - accuracy: 0.5930\n",
      "Epoch 00009: val_loss improved from 0.66409 to 0.65946, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 732us/sample - loss: 0.6616 - accuracy: 0.6009 - val_loss: 0.6595 - val_accuracy: 0.6078\n",
      "Epoch 10/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.6534 - accuracy: 0.6289\n",
      "Epoch 00010: val_loss improved from 0.65946 to 0.65166, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 713us/sample - loss: 0.6515 - accuracy: 0.6294 - val_loss: 0.6517 - val_accuracy: 0.5948\n",
      "Epoch 11/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.6441 - accuracy: 0.6453\n",
      "Epoch 00011: val_loss improved from 0.65166 to 0.64166, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 764us/sample - loss: 0.6444 - accuracy: 0.6418 - val_loss: 0.6417 - val_accuracy: 0.6144\n",
      "Epoch 12/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.6364 - accuracy: 0.6531\n",
      "Epoch 00012: val_loss improved from 0.64166 to 0.64001, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 783us/sample - loss: 0.6359 - accuracy: 0.6528 - val_loss: 0.6400 - val_accuracy: 0.6144\n",
      "Epoch 13/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.6260 - accuracy: 0.6656\n",
      "Epoch 00013: val_loss improved from 0.64001 to 0.62697, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 705us/sample - loss: 0.6261 - accuracy: 0.6623 - val_loss: 0.6270 - val_accuracy: 0.6601\n",
      "Epoch 14/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.6171 - accuracy: 0.6773\n",
      "Epoch 00014: val_loss did not improve from 0.62697\n",
      "1368/1368 [==============================] - 1s 666us/sample - loss: 0.6197 - accuracy: 0.6718 - val_loss: 0.6342 - val_accuracy: 0.6340\n",
      "Epoch 15/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.6130 - accuracy: 0.6672\n",
      "Epoch 00015: val_loss did not improve from 0.62697\n",
      "1368/1368 [==============================] - 1s 686us/sample - loss: 0.6142 - accuracy: 0.6637 - val_loss: 0.6311 - val_accuracy: 0.6471\n",
      "Epoch 16/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.6094 - accuracy: 0.6719\n",
      "Epoch 00016: val_loss improved from 0.62697 to 0.61588, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 714us/sample - loss: 0.6102 - accuracy: 0.6711 - val_loss: 0.6159 - val_accuracy: 0.6471\n",
      "Epoch 17/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.6028 - accuracy: 0.6648\n",
      "Epoch 00017: val_loss did not improve from 0.61588\n",
      "1368/1368 [==============================] - 1s 686us/sample - loss: 0.6025 - accuracy: 0.6652 - val_loss: 0.6530 - val_accuracy: 0.6209\n",
      "Epoch 18/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.6017 - accuracy: 0.6773\n",
      "Epoch 00018: val_loss improved from 0.61588 to 0.60814, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 743us/sample - loss: 0.6075 - accuracy: 0.6674 - val_loss: 0.6081 - val_accuracy: 0.6536\n",
      "Epoch 19/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.6072 - accuracy: 0.6594\n",
      "Epoch 00019: val_loss did not improve from 0.60814\n",
      "1368/1368 [==============================] - 1s 763us/sample - loss: 0.6042 - accuracy: 0.6645 - val_loss: 0.6317 - val_accuracy: 0.6471\n",
      "Epoch 20/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5958 - accuracy: 0.6703\n",
      "Epoch 00020: val_loss did not improve from 0.60814\n",
      "1368/1368 [==============================] - 1s 712us/sample - loss: 0.5959 - accuracy: 0.6747 - val_loss: 0.6331 - val_accuracy: 0.6471\n",
      "Epoch 21/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5959 - accuracy: 0.6789\n",
      "Epoch 00021: val_loss did not improve from 0.60814\n",
      "1368/1368 [==============================] - 1s 644us/sample - loss: 0.5955 - accuracy: 0.6791 - val_loss: 0.6187 - val_accuracy: 0.6536\n",
      "Epoch 22/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5891 - accuracy: 0.6805\n",
      "Epoch 00022: val_loss did not improve from 0.60814\n",
      "1368/1368 [==============================] - 1s 613us/sample - loss: 0.5901 - accuracy: 0.6820 - val_loss: 0.6380 - val_accuracy: 0.6471\n",
      "Epoch 23/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5841 - accuracy: 0.6805\n",
      "Epoch 00023: val_loss did not improve from 0.60814\n",
      "1368/1368 [==============================] - 1s 698us/sample - loss: 0.5879 - accuracy: 0.6776 - val_loss: 0.6132 - val_accuracy: 0.6601\n",
      "Epoch 24/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5932 - accuracy: 0.6789\n",
      "Epoch 00024: val_loss did not improve from 0.60814\n",
      "1368/1368 [==============================] - 1s 615us/sample - loss: 0.5880 - accuracy: 0.6813 - val_loss: 0.6275 - val_accuracy: 0.6667\n",
      "Epoch 25/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5860 - accuracy: 0.6836\n",
      "Epoch 00025: val_loss did not improve from 0.60814\n",
      "1368/1368 [==============================] - 1s 638us/sample - loss: 0.5858 - accuracy: 0.6813 - val_loss: 0.6176 - val_accuracy: 0.6601\n",
      "Epoch 26/72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5830 - accuracy: 0.6773\n",
      "Epoch 00026: val_loss did not improve from 0.60814\n",
      "1368/1368 [==============================] - 1s 767us/sample - loss: 0.5844 - accuracy: 0.6769 - val_loss: 0.6319 - val_accuracy: 0.6667\n",
      "Epoch 27/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5767 - accuracy: 0.6820\n",
      "Epoch 00027: val_loss did not improve from 0.60814\n",
      "1368/1368 [==============================] - 1s 618us/sample - loss: 0.5736 - accuracy: 0.6849 - val_loss: 0.6147 - val_accuracy: 0.6536\n",
      "Epoch 28/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5755 - accuracy: 0.6836\n",
      "Epoch 00028: val_loss did not improve from 0.60814\n",
      "1368/1368 [==============================] - 1s 574us/sample - loss: 0.5796 - accuracy: 0.6842 - val_loss: 0.6302 - val_accuracy: 0.6667\n",
      "Epoch 29/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5792 - accuracy: 0.6898\n",
      "Epoch 00029: val_loss did not improve from 0.60814\n",
      "1368/1368 [==============================] - 1s 567us/sample - loss: 0.5773 - accuracy: 0.6908 - val_loss: 0.6135 - val_accuracy: 0.6601\n",
      "Epoch 30/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5781 - accuracy: 0.6898\n",
      "Epoch 00030: val_loss did not improve from 0.60814\n",
      "1368/1368 [==============================] - 1s 785us/sample - loss: 0.5751 - accuracy: 0.6923 - val_loss: 0.6246 - val_accuracy: 0.6601\n",
      "Epoch 31/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5689 - accuracy: 0.7000\n",
      "Epoch 00031: val_loss did not improve from 0.60814\n",
      "1368/1368 [==============================] - 1s 595us/sample - loss: 0.5692 - accuracy: 0.7003 - val_loss: 0.6258 - val_accuracy: 0.6601\n",
      "Epoch 32/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5688 - accuracy: 0.7031\n",
      "Epoch 00032: val_loss did not improve from 0.60814\n",
      "1368/1368 [==============================] - 1s 686us/sample - loss: 0.5668 - accuracy: 0.7047 - val_loss: 0.6259 - val_accuracy: 0.6601\n",
      "Epoch 33/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5646 - accuracy: 0.7094\n",
      "Epoch 00033: val_loss improved from 0.60814 to 0.58874, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 597us/sample - loss: 0.5653 - accuracy: 0.7098 - val_loss: 0.5887 - val_accuracy: 0.6601\n",
      "Epoch 34/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5731 - accuracy: 0.6984\n",
      "Epoch 00034: val_loss did not improve from 0.58874\n",
      "1368/1368 [==============================] - 1s 575us/sample - loss: 0.5718 - accuracy: 0.7025 - val_loss: 0.6166 - val_accuracy: 0.6601\n",
      "Epoch 35/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5684 - accuracy: 0.7023\n",
      "Epoch 00035: val_loss did not improve from 0.58874\n",
      "1368/1368 [==============================] - 1s 566us/sample - loss: 0.5649 - accuracy: 0.7076 - val_loss: 0.6452 - val_accuracy: 0.6732\n",
      "Epoch 36/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5744 - accuracy: 0.7016\n",
      "Epoch 00036: val_loss improved from 0.58874 to 0.58852, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 594us/sample - loss: 0.5736 - accuracy: 0.6988 - val_loss: 0.5885 - val_accuracy: 0.6536\n",
      "Epoch 37/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5693 - accuracy: 0.7063\n",
      "Epoch 00037: val_loss did not improve from 0.58852\n",
      "1368/1368 [==============================] - 1s 555us/sample - loss: 0.5653 - accuracy: 0.7098 - val_loss: 0.5942 - val_accuracy: 0.6471\n",
      "Epoch 38/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5597 - accuracy: 0.7180\n",
      "Epoch 00038: val_loss did not improve from 0.58852\n",
      "1368/1368 [==============================] - 1s 571us/sample - loss: 0.5627 - accuracy: 0.7127 - val_loss: 0.6031 - val_accuracy: 0.6471\n",
      "Epoch 39/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5520 - accuracy: 0.7328\n",
      "Epoch 00039: val_loss did not improve from 0.58852\n",
      "1368/1368 [==============================] - 1s 596us/sample - loss: 0.5569 - accuracy: 0.7259 - val_loss: 0.5979 - val_accuracy: 0.6471\n",
      "Epoch 40/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5524 - accuracy: 0.7227\n",
      "Epoch 00040: val_loss did not improve from 0.58852\n",
      "1368/1368 [==============================] - 1s 653us/sample - loss: 0.5603 - accuracy: 0.7164 - val_loss: 0.6199 - val_accuracy: 0.6732\n",
      "Epoch 41/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5558 - accuracy: 0.7172\n",
      "Epoch 00041: val_loss did not improve from 0.58852\n",
      "1368/1368 [==============================] - 1s 654us/sample - loss: 0.5539 - accuracy: 0.7186 - val_loss: 0.5893 - val_accuracy: 0.6601\n",
      "Epoch 42/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5671 - accuracy: 0.7008\n",
      "Epoch 00042: val_loss improved from 0.58852 to 0.58308, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 656us/sample - loss: 0.5597 - accuracy: 0.7083 - val_loss: 0.5831 - val_accuracy: 0.6732\n",
      "Epoch 43/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5495 - accuracy: 0.7258\n",
      "Epoch 00043: val_loss did not improve from 0.58308\n",
      "1368/1368 [==============================] - 1s 625us/sample - loss: 0.5521 - accuracy: 0.7230 - val_loss: 0.5899 - val_accuracy: 0.6601\n",
      "Epoch 44/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5592 - accuracy: 0.7148\n",
      "Epoch 00044: val_loss did not improve from 0.58308\n",
      "1368/1368 [==============================] - 1s 649us/sample - loss: 0.5546 - accuracy: 0.7200 - val_loss: 0.6043 - val_accuracy: 0.6471\n",
      "Epoch 45/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5493 - accuracy: 0.7297\n",
      "Epoch 00045: val_loss improved from 0.58308 to 0.58099, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 666us/sample - loss: 0.5487 - accuracy: 0.7295 - val_loss: 0.5810 - val_accuracy: 0.6797\n",
      "Epoch 46/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5451 - accuracy: 0.7281\n",
      "Epoch 00046: val_loss did not improve from 0.58099\n",
      "1368/1368 [==============================] - 1s 635us/sample - loss: 0.5467 - accuracy: 0.7288 - val_loss: 0.6035 - val_accuracy: 0.6536\n",
      "Epoch 47/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5502 - accuracy: 0.7266\n",
      "Epoch 00047: val_loss improved from 0.58099 to 0.57615, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 659us/sample - loss: 0.5517 - accuracy: 0.7259 - val_loss: 0.5762 - val_accuracy: 0.6863\n",
      "Epoch 48/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5588 - accuracy: 0.7219\n",
      "Epoch 00048: val_loss did not improve from 0.57615\n",
      "1368/1368 [==============================] - 1s 635us/sample - loss: 0.5529 - accuracy: 0.7281 - val_loss: 0.5767 - val_accuracy: 0.6863\n",
      "Epoch 49/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5449 - accuracy: 0.7211\n",
      "Epoch 00049: val_loss did not improve from 0.57615\n",
      "1368/1368 [==============================] - 1s 651us/sample - loss: 0.5429 - accuracy: 0.7244 - val_loss: 0.5953 - val_accuracy: 0.6536\n",
      "Epoch 50/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5396 - accuracy: 0.7289\n",
      "Epoch 00050: val_loss improved from 0.57615 to 0.57587, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 687us/sample - loss: 0.5415 - accuracy: 0.7273 - val_loss: 0.5759 - val_accuracy: 0.6928\n",
      "Epoch 51/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5459 - accuracy: 0.7227\n",
      "Epoch 00051: val_loss improved from 0.57587 to 0.57327, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 659us/sample - loss: 0.5430 - accuracy: 0.7222 - val_loss: 0.5733 - val_accuracy: 0.6863\n",
      "Epoch 52/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5446 - accuracy: 0.7242\n",
      "Epoch 00052: val_loss did not improve from 0.57327\n",
      "1368/1368 [==============================] - 1s 646us/sample - loss: 0.5425 - accuracy: 0.7244 - val_loss: 0.5901 - val_accuracy: 0.6863\n",
      "Epoch 53/72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5479 - accuracy: 0.7375\n",
      "Epoch 00053: val_loss did not improve from 0.57327\n",
      "1368/1368 [==============================] - 1s 628us/sample - loss: 0.5428 - accuracy: 0.7420 - val_loss: 0.5798 - val_accuracy: 0.6797\n",
      "Epoch 54/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5417 - accuracy: 0.7437\n",
      "Epoch 00054: val_loss did not improve from 0.57327\n",
      "1368/1368 [==============================] - 1s 603us/sample - loss: 0.5362 - accuracy: 0.7471 - val_loss: 0.5750 - val_accuracy: 0.6928\n",
      "Epoch 55/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5407 - accuracy: 0.7461\n",
      "Epoch 00055: val_loss did not improve from 0.57327\n",
      "1368/1368 [==============================] - 1s 608us/sample - loss: 0.5422 - accuracy: 0.7427 - val_loss: 0.5763 - val_accuracy: 0.6928\n",
      "Epoch 56/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5384 - accuracy: 0.7352\n",
      "Epoch 00056: val_loss did not improve from 0.57327\n",
      "1368/1368 [==============================] - 1s 625us/sample - loss: 0.5323 - accuracy: 0.7398 - val_loss: 0.5740 - val_accuracy: 0.6863\n",
      "Epoch 57/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5385 - accuracy: 0.7367\n",
      "Epoch 00057: val_loss improved from 0.57327 to 0.57050, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 635us/sample - loss: 0.5403 - accuracy: 0.7354 - val_loss: 0.5705 - val_accuracy: 0.6863\n",
      "Epoch 58/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5373 - accuracy: 0.7328\n",
      "Epoch 00058: val_loss did not improve from 0.57050\n",
      "1368/1368 [==============================] - 1s 650us/sample - loss: 0.5365 - accuracy: 0.7303 - val_loss: 0.5824 - val_accuracy: 0.6993\n",
      "Epoch 59/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5358 - accuracy: 0.7477\n",
      "Epoch 00059: val_loss improved from 0.57050 to 0.56787, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 671us/sample - loss: 0.5432 - accuracy: 0.7442 - val_loss: 0.5679 - val_accuracy: 0.7124\n",
      "Epoch 60/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5302 - accuracy: 0.7500\n",
      "Epoch 00060: val_loss did not improve from 0.56787\n",
      "1368/1368 [==============================] - 1s 627us/sample - loss: 0.5353 - accuracy: 0.7434 - val_loss: 0.5717 - val_accuracy: 0.6928\n",
      "Epoch 61/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5341 - accuracy: 0.7359\n",
      "Epoch 00061: val_loss improved from 0.56787 to 0.56656, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 659us/sample - loss: 0.5329 - accuracy: 0.7383 - val_loss: 0.5666 - val_accuracy: 0.6993\n",
      "Epoch 62/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5346 - accuracy: 0.7469\n",
      "Epoch 00062: val_loss improved from 0.56656 to 0.56399, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 649us/sample - loss: 0.5334 - accuracy: 0.7471 - val_loss: 0.5640 - val_accuracy: 0.6993\n",
      "Epoch 63/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5332 - accuracy: 0.7391\n",
      "Epoch 00063: val_loss did not improve from 0.56399\n",
      "1368/1368 [==============================] - 1s 625us/sample - loss: 0.5300 - accuracy: 0.7412 - val_loss: 0.5859 - val_accuracy: 0.7059\n",
      "Epoch 64/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5280 - accuracy: 0.7461\n",
      "Epoch 00064: val_loss improved from 0.56399 to 0.56288, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 650us/sample - loss: 0.5305 - accuracy: 0.7449 - val_loss: 0.5629 - val_accuracy: 0.7059\n",
      "Epoch 65/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5410 - accuracy: 0.7359\n",
      "Epoch 00065: val_loss improved from 0.56288 to 0.56209, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 671us/sample - loss: 0.5362 - accuracy: 0.7383 - val_loss: 0.5621 - val_accuracy: 0.7059\n",
      "Epoch 66/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5382 - accuracy: 0.7430\n",
      "Epoch 00066: val_loss improved from 0.56209 to 0.56122, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 665us/sample - loss: 0.5341 - accuracy: 0.7427 - val_loss: 0.5612 - val_accuracy: 0.7059\n",
      "Epoch 67/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5304 - accuracy: 0.7375\n",
      "Epoch 00067: val_loss did not improve from 0.56122\n",
      "1368/1368 [==============================] - 1s 668us/sample - loss: 0.5291 - accuracy: 0.7390 - val_loss: 0.5614 - val_accuracy: 0.6993\n",
      "Epoch 68/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5266 - accuracy: 0.7531\n",
      "Epoch 00068: val_loss did not improve from 0.56122\n",
      "1368/1368 [==============================] - 1s 692us/sample - loss: 0.5221 - accuracy: 0.7566 - val_loss: 0.5640 - val_accuracy: 0.7190\n",
      "Epoch 69/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5337 - accuracy: 0.7375\n",
      "Epoch 00069: val_loss did not improve from 0.56122\n",
      "1368/1368 [==============================] - 1s 692us/sample - loss: 0.5324 - accuracy: 0.7383 - val_loss: 0.5627 - val_accuracy: 0.7059\n",
      "Epoch 70/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5372 - accuracy: 0.7437\n",
      "Epoch 00070: val_loss did not improve from 0.56122\n",
      "1368/1368 [==============================] - 1s 676us/sample - loss: 0.5361 - accuracy: 0.7449 - val_loss: 0.5669 - val_accuracy: 0.7320\n",
      "Epoch 71/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5388 - accuracy: 0.7477\n",
      "Epoch 00071: val_loss improved from 0.56122 to 0.55867, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "1368/1368 [==============================] - 1s 662us/sample - loss: 0.5346 - accuracy: 0.7515 - val_loss: 0.5587 - val_accuracy: 0.7124\n",
      "Epoch 72/72\n",
      "1280/1368 [===========================>..] - ETA: 0s - loss: 0.5307 - accuracy: 0.7406\n",
      "Epoch 00072: val_loss did not improve from 0.55867\n",
      "1368/1368 [==============================] - 1s 648us/sample - loss: 0.5392 - accuracy: 0.7368 - val_loss: 0.5594 - val_accuracy: 0.7255\n",
      "Training completed in time:  0:01:05.265307\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 72\n",
    "num_batch_size = 128\n",
    "\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath='saved_models/weights.best.basic_cnn.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train2, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test2, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.73976606\n",
      "Testing Accuracy:  0.7254902\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train2, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test2, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./saved_models/model5.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17864"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_model = converter.convert()\n",
    "open(\"converted_model5.tflite\", \"wb\").write(quantized_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a87813ffe34d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
